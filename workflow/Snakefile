import os
import sys
import json
import yaml

from scripts.helper_functions import (
    get_2bit_path,
    get_genome_path,
    get_index_path,
    main_annot_path)
from scripts.genomic_files_and_indexes import prepare_genomes_and_indexes

# Get parpipe2 base directory (not workflow directory)
parpipe2_dir = os.path.normpath(os.path.join(workflow.basedir, ".."))
print(parpipe2_dir)
cwd = workflow.workdir_init
# get default config and update it from current config, therefore default values can be properly used

# if config is not explicitlely defined - try to find it in the current working directory
if not config:
    print("Config file not selected")
    config_files = os.listdir(cwd)
    config_files = [x for x in config_files if x.endswith('.yml') or x.endswith('.yaml')]
    if len(config_files) == 1: #if there is only one file
        print(f"Using {config_files[0]} from {cwd} as config file")
        with open(config_files[0], encoding='utf-8') as f:
            config = yaml.safe_load(f.read())
    else:
        print(f"Multiple yml/yaml files found in the {cwd} directory, no config used.")
        print("Please explicitly define config file.")

# loading default config to use as default parameters
with open(os.path.join(parpipe2_dir, "config", "default_config.yaml"), encoding='utf-8') as f:
    params = yaml.safe_load(f.read())
params.update({k: v for k, v in config.items() if v})


# Validate required parameters
required_params = ['three_prime_adapter', 'five_prime_adapter']
for req_param in required_params:
    if params[req_param] in ["", None]:
        raise ValueError(f"{req_param} must be provided in config file")

# Construct annot_rank if not specified in config
if not params['annot_rank']:
    params['annot_rank'] = os.path.join(parpipe2_dir, "annotation", params["organism"], "annot_categories.txt")
# Construct rep_idx if not specified in config
if not params['rep_idx']:
    params['rep_idx'] = os.path.join(parpipe2_dir, "annotation", params["organism"], "rep_idx")

# save final params used for analysis
with open(os.path.join(cwd, "final_config.json"), "w") as f:
    json.dump(params,f,indent=4)

# Extract number of cores from the params
num_cores = params["threads"]
# Extract input file names from the params
input_filenames = params["input_files"]
if input_filenames:
    input_filenames = input_filenames.split(",")
else:
    input_filenames = os.listdir(cwd)
    print(input_filenames)
    input_filenames = [os.path.join(cwd, x) for x in input_filenames if not (x.endswith('.yml') or x.endswith('.yaml') or x.endswith('.json'))]
    input_filenames = [x for x in input_filenames if not os.path.isdir(x)]
if not input_filenames:
    input_filenames = [os.path.join(parpipe2_dir, "test_data", "ZFP36.fq.gz")]
    print("Running CLIPs4U with test data file")
    #raise ValueError(f"input_filenames must be provided in config file or files must be in the working directory")

# List of possible extensions
sample_names = {}
extensions = ['fq.gz', 'fq', 'fastq.gz', 'fastq']
for filename in input_filenames:
    sample_name = os.path.basename(filename).split(".")[0]
    sample_names[sample_name] = filename  # Store the full path
print("Sample names and their paths:")
print(sample_names) 

if params['umi'] == '' or int(params['umi']) == 0:
    umi_param = ""
elif int(params['umi']) > 0:
    umi_param = f"| fastx_trimmer -f {int(params['umi']) + 1}"
else:
    umi_param = f"| fastx_trimmer -t {abs(int(params['umi']))}"
    
if params["organism"] == "hs":
    eff_size = 2913022398
elif params["organism"] == "mm":
    eff_size = 2652783500
    
tar_gz_path = os.path.join(parpipe2_dir, "annotation", params["organism"], "rep_idx.tar.gz")
outdir = os.path.join(parpipe2_dir, "annotation", params["organism"])

if os.path.exists(tar_gz_path):
    os.system(f"tar -xvzf {tar_gz_path} -C {outdir}")
    print(f'Extracted repetitive elements index for {params["organism"]}')
    os.remove(tar_gz_path)
    

rule all:
    input:
        "metadata.json", # assure genome files are downloaded and genome indexes are created
        expand(f"{cwd}/qc/fastqc/{{sample}}_fastqc.html", sample=sample_names.keys()),
        expand(f"{cwd}/qc/fastqc/{{sample}}_fastqc.zip", sample=sample_names.keys()),
        #expand(f"{cwd}/reads/{{sample}}_trimmed1.fq.gz", sample=sample_names.keys()),
        #expand(f"{cwd}/reads/{{sample}}_trimmed_final.fq.gz", sample=sample_names.keys()),
        #expand(f"{cwd}/reads/{{sample}}_trimmed_collapsed.fa", sample=sample_names.keys()),
        #expand(f"{cwd}/reads/{{sample}}_rm_rep.fa", sample=sample_names.keys()),
        #expand(f"{cwd}/ali/{{sample}}.aligned.mapped.sorted.sam", sample=sample_names.keys()),
        #expand(f"{cwd}/PARalyzer/{{sample}}.ini", sample=sample_names.keys()),
        #expand(f"{cwd}/PARalyzer/{{sample}}_PARalyzer_Utilized.bam.bai", sample=sample_names.keys()),
        #expand(f"{cwd}/stats/{{sample}}_clusters_conv_stats.tsv", sample=sample_names.keys()),
        #expand(f"{cwd}/genome_viewer_files/{{sample}}_clusters_unfiltered.bed", sample=sample_names.keys()),
        expand(f"{cwd}/genome_viewer_files/{{sample}}_report.txt", sample=sample_names.keys()),
        expand(f"{cwd}/motif_enrichment/{{sample}}_report.txt", sample=sample_names.keys()),
        "final_report.html",

include: "rules/genomes_indexes.smk"
include: "rules/quality.smk"
include: "rules/trimming.smk"
include: "rules/collapse_reads.smk"
include: "rules/remove_rep_elements.smk"
include: "rules/alignment.smk"
include: "rules/paralyzer.smk"
include: "rules/annotation.smk"
include: "rules/bigwigs.smk"
include: "rules/motif_enr.smk"
include: "rules/generate_report.smk"
